{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akkirajubhavana/SMAI_2024_ASSIGNMENTS/blob/main/SMAI_A1_Q2_Faiss_Knn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imMHq8PVcEFb"
      },
      "source": [
        "## Image Captioning using KNN\n",
        "\n",
        "Although VLMs (Vision Language Models) are the go to tools for image captioning right now, there are interesting works from earlier years that used KNN for captioning and perform surprisingly well enough!\n",
        "\n",
        "Further, Libraries like [Faiss](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) perform the nearest neighbor computation efficiently and are used in many industrial applications.\n",
        "\n",
        "- In this question you will implement an algorithm to perform captioning using KNN based on the paper [A Distributed Representation Based Query Expansion Approach for\n",
        "Image Captioning](https://aclanthology.org/P15-2018.pdf)\n",
        "\n",
        "- Dataset: [MS COCO](https://cocodataset.org/#home) 2014 (val set only)\n",
        "\n",
        "- Algorithm:\n",
        "    1. Given: Image embeddings and correspond caption embeddings (5 Per image)\n",
        "    1. For every image, findout the k nearest images and compute its query vector as the weighted sum of the captions of the nearest images (k*5 captions per image)\n",
        "    1. The predicted caption would be the caption in the dataset that is closest to the query vector. (for the sake of the assignment use the same coco val set captions as the dataset)\n",
        "\n",
        "- The image and text embeddings are extracted from the [CLIP](https://openai.com/research/clip) model. (You need not know about this right now)\n",
        "\n",
        "- Tasks:\n",
        "    1. Implement the algorithm and compute the bleu score. Use Faiss for nearest neighbor computation. Starter code is provided below.\n",
        "    1. Try a few options for k. Record your observations.\n",
        "    1. For a fixed k, try a few options in the Faiss index factory to speed the computation in step 2. Record your observations.\n",
        "    1. Qualitative study: Visualize five images, their ground truth captions and the predicted caption.\n",
        "    \n",
        "Note: Run this notebook on Colab for fastest resu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0XAWcEuKvyG",
        "outputId": "34e62e22-c199-4f7d-ab0d-223dc0f636a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve file url:\n",
            "\n",
            "\tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1RwhwntZGZ9AX8XtGIDAcQD3ByTcUiOoO\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n"
          ]
        }
      ],
      "source": [
        "!gdown 1RwhwntZGZ9AX8XtGIDAcQD3ByTcUiOoO #image embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcOLiI8fgyet",
        "outputId": "3fa726b5-5984-441d-a254-48480694f3da"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "gdown.download(\"https://drive.google.com/file/d/1sXrLZgvH238JN5yJYTMQh50BjUmWwan_/view?usp=drive_link\", \"/content/coco_captions.npy\", quiet=False)\n",
        "gdown.download(\"https://drive.google.com/file/d/1RQKValuA-oGEx7s7OyGdtvz_L3tHUKSW/view?usp=drive_link\", \"/content/coco_imgs.npy\", quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "-pDwJ161ieKy",
        "outputId": "59e7c8de-9ea3-4c53-f8c3-bd4e5e68f385"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/parse_url.py:44: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=1sXrLZgvH238JN5yJYTMQh50BjUmWwan_\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/file/d/1sXrLZgvH238JN5yJYTMQh50BjUmWwan_/view?usp=drive_link\n",
            "To: /content/coco_captions.npy\n",
            "8.22kB [00:00, 11.1MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/parse_url.py:44: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=1RQKValuA-oGEx7s7OyGdtvz_L3tHUKSW\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/file/d/1RQKValuA-oGEx7s7OyGdtvz_L3tHUKSW/view?usp=drive_link\n",
            "To: /content/coco_imgs.npy\n",
            "8.22kB [00:00, 8.40MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/coco_imgs.npy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWjuVTJkDvM4",
        "outputId": "1b9839ab-1a53-4b74-dcd7-5dfa967f080e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve file url:\n",
            "\n",
            "\tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1b-4hU2Kp93r1nxMUGEgs1UbZov0OqFfW\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n"
          ]
        }
      ],
      "source": [
        "!gdown 1b-4hU2Kp93r1nxMUGEgs1UbZov0OqFfW #caption embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "P7h8e8DunDEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1dc390-7d2b-4077-9606-dde7d0070734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-01 16:36:12--  http://images.cocodataset.org/zips/val2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.234.177, 52.216.145.51, 52.217.133.137, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.234.177|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6645013297 (6.2G) [application/zip]\n",
            "Saving to: ‘val2014.zip.1’\n",
            "\n",
            "val2014.zip.1       100%[===================>]   6.19G  45.0MB/s    in 2m 46s  \n",
            "\n",
            "2024-02-01 16:38:58 (38.3 MB/s) - ‘val2014.zip.1’ saved [6645013297/6645013297]\n",
            "\n",
            "Archive:  /content/val2014.zip\n",
            "replace val2014/COCO_val2014_000000324670.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace val2014/COCO_val2014_000000464263.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N A\n",
            "A\n",
            "A\n",
            "n\n",
            "--2024-02-01 16:39:59--  http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.40.153, 52.217.204.41, 54.231.131.73, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.40.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252872794 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2014.zip.1’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.16M  43.5MB/s    in 6.1s    \n",
            "\n",
            "2024-02-01 16:40:06 (39.6 MB/s) - ‘annotations_trainval2014.zip.1’ saved [252872794/252872794]\n",
            "\n",
            "Archive:  /content/annotations_trainval2014.zip\n",
            "replace annotations/instances_train2014.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n"
          ]
        }
      ],
      "source": [
        "!wget http://images.cocodataset.org/zips/val2014.zip\n",
        "!unzip /content/val2014.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "!unzip /content/annotations_trainval2014.zip\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jI9PuLZ6b51J"
      },
      "outputs": [],
      "source": [
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nltk.translate import bleu_score\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nltk.translate import bleu_score\n",
        "import faiss\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igqtoD36iQv8",
        "outputId": "7e021cf1-0931-420b-c158-63aae4591b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.43s)\n",
            "creating index...\n",
            "index created!\n",
            "Number of samples:  40504\n",
            "Image Size:  torch.Size([3, 224, 224])\n",
            "['A loft bed with a dresser underneath it.', 'A bed and desk in a small room.', 'Wooden bed on top of a white dresser.', 'A bed sits on top of a dresser and a desk.', 'Bunk bed with a narrow shelf sitting underneath it. ']\n"
          ]
        }
      ],
      "source": [
        "def get_transform():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
        "        transforms.Normalize(\n",
        "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
        "            (0.229, 0.224, 0.225),\n",
        "        )\n",
        "    ])\n",
        "    return transform\n",
        "\n",
        "coco_dset = dset.CocoCaptions(root = '/content/val2014',\n",
        "                        annFile = '/content/annotations/captions_val2014.json',\n",
        "                        transform=get_transform())\n",
        "\n",
        "print('Number of samples: ', len(coco_dset))\n",
        "img, target = coco_dset[3] # load 4th sample\n",
        "\n",
        "print(\"Image Size: \", img.shape)\n",
        "print(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpjbQ_KUick0",
        "outputId": "a4d9ab22-75f0-4d40-d79b-12d1f9b41fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captions: (40504, 5)\n"
          ]
        }
      ],
      "source": [
        "ids = list(sorted(coco_dset.coco.imgs.keys()))\n",
        "captions = []\n",
        "for i in range(len(ids)):\n",
        "    captions.append([ele['caption'] for ele in coco_dset.coco.loadAnns(coco_dset.coco.getAnnIds(ids[i]))][:5]) #5 per image\n",
        "captions_np = np.array(captions)\n",
        "print('Captions:', captions_np.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alKCWuRhivc_",
        "outputId": "5add4483-d114-4690-9db2-a0e7e4d22fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total captions: 202520\n"
          ]
        }
      ],
      "source": [
        "captions_flat = captions_np.flatten().tolist()\n",
        "print('Total captions:', len(captions_flat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leEngJ4hkzpD",
        "outputId": "51945b28-e311-4208-b820-17ce41248187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption embeddings (40504, 5, 512)\n"
          ]
        }
      ],
      "source": [
        "cap_path = '/content/drive/MyDrive/coco_captions.npy'\n",
        "caption_embeddings = np.load(cap_path)\n",
        "print('Caption embeddings',caption_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnAdVULXlTNV",
        "outputId": "76da025d-372f-42e3-b081-9f1048e38768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image embeddings (40504, 512)\n"
          ]
        }
      ],
      "source": [
        "img_path = '/content/drive/MyDrive/coco_imgs.npy'\n",
        "image_embeddings = np.load(img_path)\n",
        "print('Image embeddings',image_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(predict, real):\n",
        "    '''\n",
        "    use bleu score as a measurement of accuracy\n",
        "    :param predict: a list of predicted captions\n",
        "    :param real: a list of actual descriptions\n",
        "    :return: bleu accuracy\n",
        "    '''\n",
        "    accuracy = 0\n",
        "    for i, pre in enumerate(predict):\n",
        "        references = real[i]\n",
        "        score = bleu_score.sentence_bleu(references, pre)\n",
        "        accuracy += score\n",
        "    return accuracy/len(predict)"
      ],
      "metadata": {
        "id": "fl3UFKRx3ILl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KeIe1UowsZSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lcIR0VxprV9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vector_a, vector_b):\n",
        "    dot_product = np.dot(vector_a, vector_b)\n",
        "    norm_a = np.linalg.norm(vector_a)\n",
        "    norm_b = np.linalg.norm(vector_b)\n",
        "    similarity = dot_product / (norm_a * norm_b)\n",
        "    return similarity\n"
      ],
      "metadata": {
        "id": "GqwJUwn9FbIg"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have your image_embeddings, caption_embeddings, and captions_np defined\n",
        "x_train, x_test, y_train, y_test, y_captions_train, y_captions_test = train_test_split(\n",
        "    image_embeddings, caption_embeddings, captions_np, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "Xv1ubp_Qf-Lt"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.IndexFlatL2(512)\n",
        "index.add(x_train)\n",
        "print(index.is_trained)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hknGxrNf9LK",
        "outputId": "9fbd3710-7d25-48b7-a5f2-e5844bc32ce6"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query_expansion_with_indices(x_test_instance, x_train, caption_embeddings, index, k):\n",
        "    D, I = index.search(np.reshape(x_test_instance, (-1, 512)), k)\n",
        "    query_vector = np.zeros(512, dtype=np.float32)\n",
        "    Z = np.sum(D[0])\n",
        "\n",
        "    for i in range(len(D)):\n",
        "        similarity_score = 1 - (D[0][i] / Z)\n",
        "        nearest_neighbor_indices = I[0][i]\n",
        "        nearest_neighbor_embeddings = caption_embeddings[nearest_neighbor_indices]\n",
        "\n",
        "        query_vector += np.sum(similarity_score.reshape(-1, 1) * nearest_neighbor_embeddings, axis=0)\n",
        "\n",
        "    query_vector = (1 / (k * 5)) * query_vector\n",
        "    return query_vector, nearest_neighbor_indices\n"
      ],
      "metadata": {
        "id": "GZFs1tIi9aFD"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def re_rank_candidates_with_indices(candidates, query_vector, caption_embeddings, I, y_captions_train, nearest_neighbor_indices):\n",
        "    all_captions_instances = np.reshape(candidates, (-1, 512))\n",
        "    similarities = np.array([cosine_similarity(query_vector, instance) for instance in all_captions_instances])\n",
        "    sorted_idx = np.argsort(similarities)\n",
        "    all_captions_text = y_captions_train[I].flatten()\n",
        "\n",
        "    # Return both the top-ranked caption and its similarity score\n",
        "    top_ranked_caption = all_captions_text[sorted_idx[0]]\n",
        "    top_similarity_score = similarities[sorted_idx[0]]\n",
        "\n",
        "    return top_ranked_caption, top_similarity_score"
      ],
      "metadata": {
        "id": "Wqrmt2Hr9cbo"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_similarity(x_test, x_train, caption_embeddings, index, k, y_captions_train, y_captions_test):\n",
        "    predictions = []\n",
        "    query_captions = []  # To store captions of the queries\n",
        "    top_ranked_captions = []  # To store captions of the top-ranked candidates\n",
        "\n",
        "    for i, instance in enumerate(x_test):\n",
        "        query_vector, nearest_neighbor_indices = query_expansion_with_indices(instance, x_train, caption_embeddings, index, k)\n",
        "        D, I = index.search(np.reshape(instance, (-1, 512)), k)\n",
        "        candidates = caption_embeddings[I[0]]\n",
        "\n",
        "        # Use re_rank_candidates_with_indices to get the top-ranked caption and its similarity score\n",
        "        top_caption, top_similarity = re_rank_candidates_with_indices(candidates, query_vector, caption_embeddings, I, y_captions_train, nearest_neighbor_indices)\n",
        "        predictions.append((top_caption, top_similarity))\n",
        "\n",
        "        # Store captions of the query and top-ranked candidate\n",
        "        query_captions.append(y_captions_test[i])\n",
        "        top_ranked_captions.append(top_caption)\n",
        "\n",
        "    return predictions, query_captions, top_ranked_captions"
      ],
      "metadata": {
        "id": "_PQzdtHT9gWY"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k= 5\n",
        "\n",
        "predictions, query_captions, top_ranked_captions=predict_with_similarity(x_test, x_train, caption_embeddings, index, k, y_captions_train, y_captions_test)"
      ],
      "metadata": {
        "id": "VEdGaH_HuteZ"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_accuracy_top_vs_query = accuracy(top_ranked_captions, query_captions)\n",
        "print(f\"BLEU Accuracy (Top vs. Query): {bleu_accuracy_top_vs_query * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYUi3tb_Sa9A",
        "outputId": "74f970cb-6fe5-48e8-a540-c2e4e01d949a"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Accuracy (Top vs. Query): 54.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = [3, 5, 7,10,20]  # Try different values of k\n",
        "\n",
        "for k_value in k_values:\n",
        "    predictions, query_captions, top_ranked_captions=predict_with_similarity(x_test, x_train, caption_embeddings, index, k_value, y_captions_train, y_captions_test)\n",
        "\n",
        "    bleu_accuracy = accuracy(top_ranked_captions, query_captions)\n",
        "    print(f\"For k={k_value}, BLEU Accuracy: {bleu_accuracy}\")\n"
      ],
      "metadata": {
        "id": "wTtokMaojQHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "index_factory_strings = [\"PCA32,Flat\", \"PCA64,Flat\", \"PCA80,Flat\", \"HNSW32\", \"OPQ16_64,IMI2x8,PQ8+16\"]\n",
        "k = 10\n",
        "\n",
        "for index_factory_string in index_factory_strings:\n",
        "    start_time = time.time()\n",
        "    index = faiss.index_factory(512, index_factory_string)\n",
        "    index.train(x_train)\n",
        "\n",
        "    predictions, references = predict_with_similarity(\n",
        "        x_test, x_train, caption_embeddings, index, k, y_captions_train, y_captions_test\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    accuracy_value = accuracy(predictions, references)\n",
        "\n",
        "    print(\"Execution with\", index_factory_string, \"is\", end_time - start_time, \"seconds and accuracy is\", accuracy_value)\n"
      ],
      "metadata": {
        "id": "G4a2CqAVUdf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Stt6cKohPi3B"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Replace index_to_visualize with the desired index\n",
        "index_to_visualize = 0\n",
        "\n",
        "# Get image and captions\n",
        "img, captions = x_test[index_to_visualize], y_captions_test[index_to_visualize]\n",
        "\n",
        "# Assuming predictions contain (top_ranked_caption, top_similarity) tuples\n",
        "predicted_caption, similarity_score = predictions[index_to_visualize]\n",
        "\n",
        "# Plot the image\n",
        "plt.imshow(img)\n",
        "plt.title(\"Image\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Display ground truth captions\n",
        "print(\"Ground Truth Captions:\")\n",
        "for caption in captions:\n",
        "    print(\"- \", caption)\n",
        "\n",
        "# Display predicted caption\n",
        "print(\"\\nPredicted Caption:\", predicted_caption)\n",
        "print(\"Similarity Score:\", similarity_score)\n"
      ],
      "metadata": {
        "id": "UkXg3AOGDk4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WhWXnpdLTigx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g7158baUyvT",
        "outputId": "4c3a4c84-5c07-42b5-fb7b-a27e00a277db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "* As k value increases, accuracy decreased\n"
      ],
      "metadata": {
        "id": "BYm_pWxl4yfI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}